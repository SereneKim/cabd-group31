---
title: "Anti-Vaccination & Vaccine Hesitancy: Where do they come from?"
author: "Group 31: Sangqi Pan, Seorin Kim, & Yisheng Huang "
date: "`r Sys.Date()`"
output:
        rmdformats::downcute:
          self_contained: TRUE
          thumbnails: FALSE
          lightbox: FALSE
          gallery: FALSE
          highlight: tango
          fig_width: 8
          toc_depth: 3
          number_sections: yes
bibliography: references.bib
link-citations: TRUE
---

```{css, echo=FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 300px;
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
devtools::source_gist("c83e078bf8c81b035e32c3fc0cf04ee8", 
                      filename = 'render_toc.R')
```

![](https://www.cdc.gov/coronavirus/2019-ncov/images/vaccines/HCW_vaccine_page_transp-300x236-1.png?_=13435 "covidvaccine"){width="464"}

The widespread prevalence of COVID-19 has urged countries to eradicate the virus by vaccinating their citizens. However, such a large-scale vaccination plan has encountered hesitancy and opposition from many people. The fear, doubts, and suspicions around the vaccine have appeared not only among the general citizens but also among the medical workers such as doctors and nurses [e.g., @browne2021coronavirus]. For instance, a survey shared by the Centers for Disease Control and Prevention found that 63% of surveyed health care workers reported they would not be willing to accept a COVID-19 vaccination \[@gorfinkel2022\]. Given this, we aim to answer the research question: 'Why are people unwilling to take or even against COVID-19 vaccines?' In doing so, we hope to find some variables that can be useful in improving vaccination rates.

The Oxford English dictionary defines an *anti-vaxxer* as "a person who is opposed to vaccination" [@stevenson2010\]. Some researchers have also characterized *vaccination hesitancy* as a more polite usage of the term *anti-vaccination* \[e.g., @razai2021\]. However, some researchers argue that anti-vaxxers and those with vaccine hesitancy should be considered as two different groups - the former being those who expect a scientific response and the latter promoting reductionist and ignore science \[e.g., @berman2020; @gorfinkel2022b]. In the current research, we agree with the first argument that treats *anti-vaccination* and *vaccine hesitancy* as a same concept. Therefore, we will use these two terms interchangeably in this project.

Our current research provides an answer to the main research question by exploring **two sub-questions**, using two types of data (i.e. Twitter data and survey data).

In Study 1, with **Twitter** data from 01 October 2020 to 31 March 2021, we will explore ***what topics are associated with the anti-vaccination sentiment*** using an unsupervised topic model. The Latent Dirichlet Allocation (LDA) method will be used to construct the topic model. This analysis allows a glimpse of the characteristics of anti-vaccination sentiment - namely, where the negative sentiments about COVID-19 vaccination arise. With the insights we gather from Study 1, we further investigate ***the*** ***variables that may contribute to people's hesitation toward vaccination*** through survey data collected from the EU-27. The supervised machine learning methods, decision tree and random forest, will be employed. 

As this survey data was collected between 15 February 2021 and 30 March 2021, we will be able to combine the insights with the ones from the Twitter data to obtain a more continuous inference. Additionally, this survey data not only provide responses to more specific questions regarding one's attitude towards COVID-19 vaccines but also socioeconomic and demographic data about the respondents, which are not available in the Twitter data. This richness of the data serves us to draw a better classification model to discover what makes one hesitant to vaccinate.

Conclusively, we aim to explore two different types of 'big' data - Twitter data and (big) survey data - throughout this research. As we learned about the strong- and weak points of the two from the literature review (Assignment 1), we analyze the two types of data with the main research question in mind. We hope our research can shed a light on the current issue of vaccine hesitancy and help us understand the reasons behind people's reluctance or opposition to the COVID-19 vaccines.

# Table of Contents

```{r toc, echo=FALSE, warning=FALSE, message=FALSE}
render_toc("Twitter_Final.Rmd")
```

# Study 1: NLP on Twitter Data

The data sets come from @muric2021a who have been collecting the anti-vaccination Twitter data since the beginning of the COVID-19 outbreak. Two types of data have been collected and made publicly available: streaming and account tweets. The former contains tweets collected based on keywords related to anti-vaccination, and the latter targets anti-vaxxer accounts and gives all the posts of those targeted accounts. Between the two types of data sets, we work with the streaming data from October 2020 to March 2021. Although the account data may give us more insights into the general characteristics of anti-vaccination sentiment, the size of the account data is too large to handle for the computational power we own. Moreover, as it includes all the tweets certain accounts posted, there is a lot of noise in the data which can lead to undesirable results for the research question. Hence, the streaming data is preferred as it involves smaller data sets and allows us to focus on the research question of the characteristics of anti-vaccination sentiment.

In the following analysis, we use the streaming data of October 2020 to March 2021 which are the first 6 month period of collected data and where the COVID vaccine started being tested and prepared for distribution [@cnncovid]. We selected these data sets assuming that the anti-vaccination sentiment was more strongly expressed on social media at the beginning of the distribution than later.

Due to the large data size, we will first focus on the tweets of October 2020 and November 2020. Then we will delve further into the rest of the data sets. Note here that, despite a large number of observation, we do not use a sampling method since each observation only contains maximum 280 characters and we have not encountered any problems running the following codes with the amount of data we have.

## Setup

### Packages

```{r packages_nlp, message=FALSE, warning=FALSE}
library(glue)
library(dplyr)
library(tidyverse) #need 'stringr' package which is included in 'tidyverse'; general utility & workflow functions
library(tidytext) # tidy implimentation of NLP methods
library(ggplot2)
library(gridExtra) # for ggplot arrangement
library(tibble)
library(broom)

#Tokenization
library(quanteda) 
library(quanteda.textstats)
library(quanteda.textplots)

#Lemmatization
library(textstem)

#Natural Language Processing
library(tm) # general text mining functions, making document term matrices
library(seededlda)
library(DT)

library(rtweet) # To get emojis dataset
```

### Loading Data

First we will download all the data and divide them into different data frames per month. Following the privacy policies, only Tweet IDs have been shared in the Git repository of @muric2021a. Therefore, we used `Hydrator` to hydrate the obtained Tweet IDs and created cvs files.

```{r data}
setwd("~/serene2/cabd_group31")

data_all <- lapply(Sys.glob(paths = "202*-*-merged.csv"), read.csv)
```

```{r}
oct <- as.data.frame(data_all[1])
nov <- as.data.frame(data_all[2])
dec <- as.data.frame(data_all[3])
jan <- as.data.frame(data_all[4])
feb <- as.data.frame(data_all[5])
mar <- as.data.frame(data_all[6])

data <- rbind(oct, nov) # Only use oct&nov due to big size for the first part
```

The data contains 35 variables e.g., coordinates, hashtags, urls, id, etc. As we only need the contents that people posted, we take only the Tweets from the data.

```{r}
tweets <- data[,'text']
```

## LDA on Oct & Nov Data Sets

In this section, we perform an *unsupervised* topic modeling on the October and November data sets with LDA method. Firstly, we preprocess and normalize the texts such that the textual data can be easily comprehended by the LDA algorithm. Then, in the second part, we will explore the frequently appeared terms which enlighten the possible topics emerging from the data. Lastly, we present the LDA results together with its visualization.

### Preprocessing

#### Data Cleaning

Tweets contain contractions, abbreviations, emojis, non-alphanumeric characters, inter alia, that disturb the topic model to yield interpretable results. To focus on what really contributes to forming topics, we disregard the following types of texts:

-   ID-Tags and Hashtags

-   URL

-   Emojis and Punctuations

-   Non-alphabetic letters (e.g., Han, Hangul, Hiragana, Katakana, Arabic, Devanagari, etc.)

-   Numbers

-   HTML codes and other trailing white space

The following function will remove the majority of the words and characters that are not important for the analysis. However, given that many languages use Latin alphabets and we do not limit the scope to a specific location, non-English terms may appear predominant in the following topic analysis. For those, we will have to run the codes and remove the frequently appearing non-English words manually.

```{r clean_fun}
# Function to remove unnecessary words/letters
clean <- function (contents){
        # Unifying the curly/straight quotes into straight ones
        contents <- gsub("[“”]", "\"", gsub("[‘’]", "'", contents))
        contents <- iconv(contents, "latin1", "ASCII", sub="") # no non-latin alphabets
        contents %>% 
                tolower() %>% # to lower case 
                str_remove_all("(^|\\s)([@#][\\w_-]+)") %>% # remove id-tags & hashtags
                str_remove_all("(https?://\\S+)") %>%  # remove the links
                str_replace_all("'s", " ") %>% # Removing 's e.g., today's
                str_replace_all("[:punct:]", " ") %>%  # remove punctuations
                str_replace_all("~", " ") %>% # [:punct:] doesn't remove tilde
                str_replace_all("[:digit:]", " ") %>%
                str_remove_all("[:emoji:]") %>% # No more emojis! (Just in case!)
                str_replace_all("[^A-Za-z0-9]", " ") %>% # Non alphanumeric letters
                str_replace_all("\\s+", " ") %>% # replace HTML tag \n for new lines
                trimws() # trim the leading and/or trailling whitespace
}

# Apply the function to the data
cleaned_tweets <- clean(tweets)

# sum(str_detect(cleaned_tweets, "निर्मल") == TRUE)
head(cleaned_tweets, 10)
```

```{r}
# Adding the cleaned text into the dataframe
data$cleaned_tweets <- cleaned_tweets
```

#### Creating Corpus

Once we have a cleaned data set, we create a corpus of the textual data to which we will employ a language model later.

```{r corpus}
corp_tweets <- corpus(data, text_field = "cleaned_tweets")
```

#### Lemmatization

With the corpus, we lemmatize the words. Thus, e.g., `better` becomes `good`, `has` becomes `have`, and so on.

```{r lemma}
lem_tweets <- lemmatize_strings(corp_tweets)
```

#### Tokenization

Once the lemmatized, we tokenize the corpus of the text data into words. Here, we remove all the stop words. Often, `stopwords("english")` is used but we opted for `SMART` as it contains more stop words than the former. As mentioned in Section \@ref(data-cleaning), we also added other words that we saw appearing even after removing the stop words such as `bc` (the abbreviation of the term, *because*), `don` (a part of *don't*), and French articles, `de` and `la`.

Note that the stop words include negations - e.g., don't, not, - which might be influential for sentiment analysis. But for the topic modeling, it is preferred to remove the negations.

```{r token}
# Tokenization 
toks_tweets <- quanteda::tokens(lem_tweets, what="word", remove_numbers = T, remove_symbols = T)

# stopwords("english")
# stopwords("SMART") # It has more words than "english"

toks_tweets_nostop <- tokens_remove(toks_tweets, c(stopwords("SMART"), "bc", "don", "isn", "aren", "de", "la"))
# head(toks_tweets_nostop)
```

### Exploratory Analysis

#### Frequent Words

With the preprocessed data, we can already look at the frequent words. This allows to already imagine the topics that emerge from the tweets we gathered.

```{r word_freq_fn}
# Function for word frequency
word_fq <- function(toks){
        fq <- as_tibble(unlist(toks))
        fq %>% 
                group_by(value) %>%
                summarise(n = n()) %>%
                arrange(desc(n))}



DT::datatable(word_fq(toks_tweets_nostop), class = "display", rownames = FALSE, options = list(pageLength = 5, autoWidth = FALSE))
```

The top 3 occurring words are `depopulation`, `vaccine`, and `amp` (which may stand for *rich, money, power* as suggested in Urban Dictionary [@ampmeaning]). Linking with words like `gate` which probably comes from `Bill Gates` and `agendum` and `plan`, we can already guess that many tweets may concern the manipulation of the population by the elites in order to depopulate the world.

#### Unigrams + WordCloud

The frequent words can be visualized as a word cloud.

```{r}
# Unigrams
unigram_toks <- tokens_ngrams(toks_tweets_nostop, n=1)
unigram_dfm <- dfm(unigram_toks)
unigram_freq <-textstat_frequency(unigram_dfm)

# Plot wordcloud to show most frequent words
textplot_wordcloud(unigram_dfm, max_words = 200, ordered_color = TRUE)
```

-   The most frequent word is `depopulation` and then `vaccine` and `amp` follow the next as we saw in the table above.

#### Bigrams + WordCloud

Frequent words or unigrams normally consider each word individually. However, sometimes it is more meaningful to look at a set of words together. For instance, when `Bill` appeared in the unigram word cloud, one may think that there were tweets talking about a new law or amount of money one owes to someone, while seeing *Bill* together with *Gates* gives a different interpretation of the tweets. For this reason, we look at the Bigrams and the word cloud they create.

```{r}
# Bigrams
bigram_toks <- tokens_ngrams(toks_tweets_nostop, n=2)
bigram_dfm <- dfm(bigram_toks)
bigram_freq <-textstat_frequency(bigram_dfm)

# bigram_freq
# Plot wordcloud to show most frequent words
textplot_wordcloud(bigram_dfm, max_words = 100,
                   min_size = 0.7, max_size = 3,
                   ordered_color = TRUE)
```

-   `Bill_Gate` , `Depopulation_Agendum` (i.e., lemma of Agenda), and `Great_Reset` are top 3 of the most frequent words. Again, we can interpret that there is a sentiment that 'elite such as Bill Gates trying to de-populate the world'.

#### Trigrams

Trigrams are also possible for the same reason as for bigrams. Here, we did not ask for word cloud since it took long to load it and the result did not seem as much insightful as with the bigrams. Instead, we look at the frequency table.

```{r}
# Trigrams
trigram_toks <- tokens_ngrams(toks_tweets_nostop, n=3)
trigram_dfm <- dfm(trigram_toks)
trigram_freq <-textstat_frequency(trigram_dfm)


DT::datatable(trigram_freq, class = "display", rownames = F, options = list(pageLength = 5, autoWidth = FALSE))
```

-   While a similar conclusion is possible, we also see terms like `slave_british_colony` and `depopulation_hap_obigbo`. Although it is difficult to grasp the full picture with only key words, it can be understood that some people linked the depopulation conspiracy of COVID-19 to the arrest of Obigbo residents by the soldiers in Nigeria [see @obigbo].

#### Document-feature Matrix Plot

Document-feature matrix is another way to explore the frequent words. As we did it for word clouds, we can also create these plots for uni- and bigrams.

```{r}
# Plot it
unigram_dfm %>% # Unigram dfm created before
  textstat_frequency(n = 30) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Frequency") +
  theme_minimal()
```

```{r}
# With Bigram (More informative)
bigram_dfm %>% 
  textstat_frequency(n = 30) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Frequency") +
  theme_minimal()
```

### LDA (Unsupervised Text Modelling)

Now we apply the LDA algorithm to the text data we preprocessed. For this, we use a document-feature matrix which we already created with Section \@ref(unigrams-wordcloud). Moreover, we trim the document feature matrix such that the words that mentioned too many times or too fewer times will be neglected in the analysis. Extremely high or low frequency of words implies their low importance in the content. Given the word limits on Tweets and the large number of the 'documents' (i.e., Tweets), we only apply the limits based on the proportion of each term across documents using the arguments of `min_docfreq` and `max_docfreq`, instead of limiting an exact minimum/maximum number of words occurring by `min_termfreq` / `max_termfreq`. Accordingly, we keep the terms that occur at least in 1 of 100 documents and at most, in 4 of 5 documents.

```{r}
dfmtrim <- unigram_dfm %>% 
              dfm_trim(min_docfreq = 0.01, max_docfreq = 0.8, docfreq_type = "prop")

set.seed(0)
tweets_lda <- textmodel_lda(dfmtrim, k = 10) # 10 topics
```

```{r}
DT::datatable(terms(tweets_lda), class = "display", rownames=FALSE, options = list(pageLength = 10, autoWidth = FALSE))
```

The LDA yields `phi` and `theta` where the former gives the distribution of words for topic *k*, and the latter produces the distribution of topics for document *i.* We will utilize `phi` values to visualize what terms are highly engaged with each topic.

```{r}
# tweets_lda$phi # between topic and words
# tweets_lda$theta # between each text and topic
# class(tweets_lda$phi)

lda_results <- as.data.frame(t(tweets_lda$phi))
DT::datatable(lda_results, class = "display", rownames = TRUE, options = list(pageLength = 5, autoWidth = FALSE))
```

#### Plotting

Based on the phi values from the LDA, we plot the values with the high weight for each topic.

```{r top_fun, message=TRUE, warning=FALSE}
results2 <- as_tibble(lda_results, rownames = NA) %>% 
                rownames_to_column(var = 'words') 

top_results <- function(results2){
        for (topic in colnames(results2[2:11])) {
                top <- results2[, c('words', topic)] %>% 
                        slice_max(results2[, topic], n=10)
                # top <- results2[, c('words', topic)] %>% 
                #         top_n(10, results2[, topic])
        assign(paste0('top', topic), top, envir = parent.frame())}
}

top_results(results2)
```

```{r plt_func_lda }
# Plotting per topic    

# Somehow errors occurred constantly when using for-loop with ggplot. Hence, we wrote a less elegant function with all the elements in it. 

plt_fn <- function(toptopic1, toptopic2, toptopic3, toptopic4, toptopic5, toptopic6, toptopic7, toptopic8, toptopic9, toptopic10){
        p1 <- ggplot(data=toptopic1, aes(words, topic1, fill = "orange")) +
                        geom_col(show.legend = FALSE) + # as a bar plot
                        labs(x = NULL, y = NULL, title="Topic 1") +
                        coord_flip() 
        p2 <- ggplot(data=toptopic2, aes(words, topic2, fill = "orange")) +
                        geom_col(show.legend = FALSE) + # as a bar plot
                        labs(x = NULL, y = NULL, title="Topic 2") +
                        coord_flip() 
        p3 <- ggplot(data=toptopic3, aes(words, topic3, fill = "orange")) +
                        geom_col(show.legend = FALSE) + # as a bar plot
                        labs(x = NULL, y = NULL, title="Topic 3") +
                        coord_flip() 
        p4 <- ggplot(data=toptopic4, aes(words, topic4, fill = "orange")) +
                        geom_col(show.legend = FALSE) + # as a bar plot
                        labs(x = NULL, y = NULL, title="Topic 4") +
                        coord_flip() 
        p5 <- ggplot(data=toptopic5, aes(words, topic5, fill = "orange")) +
                        geom_col(show.legend = FALSE) + # as a bar plot
                        labs(x = NULL, y = NULL, title="Topic 5") +
                        coord_flip() 
        p6 <- ggplot(data=toptopic6, aes(words, topic6, fill = "orange")) +
                        geom_col(show.legend = FALSE) + # as a bar plot
                        labs(x = NULL, y = NULL, title="Topic 6") +
                        coord_flip() 
        p7 <- ggplot(data=toptopic7, aes(words, topic7, fill = "orange")) +
                        geom_col(show.legend = FALSE) + # as a bar plot
                        labs(x = NULL, y = NULL, title="Topic 7") +
                        coord_flip()
        p8 <- ggplot(data=toptopic8, aes(words, topic8, fill = "orange")) +
                        geom_col(show.legend = FALSE) + # as a bar plot
                        labs(x = NULL, y = NULL, title="Topic 8") +
                        coord_flip()
        p9 <- ggplot(data=toptopic9, aes(words, topic9, fill = "orange")) +
                        geom_col(show.legend = FALSE) + # as a bar plot
                        labs(x = NULL, y = NULL, title="Topic 9") +
                        coord_flip()
        p10 <- ggplot(data=toptopic10, aes(words, topic10, fill = "orange")) +
                        geom_col(show.legend = FALSE) + # as a bar plot
                labs(x = NULL, y = NULL, title="Topic 10") +
                        coord_flip()

        grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10,  nrow = 2, newpage=TRUE )
        }

plt_fn(toptopic1, toptopic2, toptopic3, toptopic4, toptopic5, toptopic6, toptopic7, toptopic8, toptopic9, toptopic10)
```

First we see that the term `depopulation` appears in all the topics except Topic 10. Focusing on the unique and frequent words, the following terms can describe each topic:

-   Topic 1: Million, People, Good, Life, Good, Die
-   Topic 2: World, Global, Depopulation, Plan, Surveillance
-   Topic 3: Depopulation, Kill, Game, Lock-down, Systematic, Torture
-   Topic 4: Bill Gates, Depopulation, DNA, Vaccine
-   Topic 5: Vaccine, Covid, Mask, Test, Force,
-   Topic 6: AMP, Agendum, NWO\*, Elite, Evil, Mass, Control
-   Topic 7: People, Great, Depopulation, Climate Change, Resource, Reset
-   Topic 8: Depopulation, Agendum, Numb, Happy, Truth, Slave, British Colony
-   Topic 9: Depopulation, Mass, Control, Health, World
-   Topic 10: Vaxxed, Vax, Child, Find, Study

\*NWO means "a conspiracy theory which hypothesizes a secretly emerging totalitarian world government" [@wikinwo].

Given that all negative terms such as `not`, `'t` and the general trend of the terms appeared in the plots, even some positive terms like `good` and `happy` cannot be interpreted positively. Generally, the message that 'COVID-19 and COVID-19 vaccines are part of the depopulation agenda of the elites' resonate through all the topics. On top of that, Topic 3 and Topic 5 denote people's negative view on the COVID-measures including wearing masks, lock-downs, and more. Such a negative view on the measures seems to be linked to the belief that global elites are trying to control people. When looking at Topic 7, we also notice that some may relate COVID-19 to depopulation agenda together with Climate Change as these terms appear to be engaged in the same topic. Despite minor differences in emphasis in each topic, many terms reappear in many topics and the extracted topics form a coherent theme of the Tweets of our interest - namely, conspiracy theory.

### Hash-tags

Lastly, we can look at the hash-tags and how they are related in order to extract more insights into the topics we got. Note here that as the tweets were collected with a list of key words (i.e., hash-tags), those words may appear more frequently. Nonetheless, plotting the relations between the hash-tags allows some general topics discussed in the tweets.

```{r}
hashtags <- data[,'hashtags']
head(hashtags)
```

```{r}
hashtags_corpus <- corpus(hashtags)
hashtags_dfm <- quanteda::tokens(hashtags_corpus, remove_punct = TRUE) %>% dfm()
# head(hashtags_dfm)
```

```{r}
set.seed(1)
toptag <- names(topfeatures(hashtags_dfm, 50))
tag_fcm <- fcm(hashtags_dfm)
```

```{r}
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag)
textplot_network(topgat_fcm, min_freq = 0.1, edge_alpha = 0.8, edge_size = 5)
```

Based on the unsupervised topic modelling, we see that most tweets relate to the fear of being controlled by the so-called 'elites'. Also by plotting the hash-tags, we see 2 big chunks - (1) The one on the left hand side expresses people's opposition to the restrictions such as lock down and masks; (2) The one on the right hand side mainly concerns blaming the 'elites' when talking about COVID. Nevertheless, a distinct difference between the two chunks is not possible and similar hash-tags appear in both blobs.

## LDA with All Data

Since we did not see much distinct kinds of topics with October 2020 and November 2020 data, we aim to verify whether this is still the case when we extend the scope of the data, involving the data all the way to March 2021. Although such data contains 359688 observations, it is still doable to run the topic model since the lengths of the texts are all short.

### Data Preprocessing

#### Data Cleaning

We bind the monthly separated data frames into one. Note that `data_all` consists of 6 lists of data frames, which is not appropriate for analyzing LDA on all the tweets across the months.

```{r}
data_all_in <- rbind(oct, nov, dec, jan, feb, mar)
```

Taking only the Tweets from the data.

```{r}
tweets_all <- data_all_in[, 'text']
```

```{r}
# Remove unnecessary words/letters
cleaned_tweets_all <- clean(tweets_all) 
data_all_in$cleaned_tweets_all <- cleaned_tweets_all
```

#### Data Preparation for LDA

We do the same preprocessing as in Section \@ref(preprocessing).

```{r}
#Corpus
corp_tweets_all <- corpus(data_all_in, text_field = "cleaned_tweets_all")

#Lemmatization
lem_tweets_all <- lemmatize_strings(corp_tweets_all)
```

```{r}
# Tokenization 
toks_tweets_all <- quanteda::tokens(lem_tweets_all, what="word", remove_symbols=T)

toks_tweets_nostop_all <- tokens_remove(toks_tweets_all, c(stopwords("SMART"), "bc", "don", "isn", "aren", "de", "la"))

head(toks_tweets_nostop_all)
```

### Frequent Words

We will only look at the frequent words in this part.

```{r}
DT::datatable(word_fq(toks_tweets_nostop_all), class = "display", rownames=FALSE, options = list(pageLength = 5, autoWidth = FALSE))
```

### LDA

```{r worldcloud_all}
dfm_all <- dfm(toks_tweets_nostop_all)

# Plot wordcloud to show most frequent words
textplot_wordcloud(dfm_all, max_words = 200, ordered_color = TRUE)
```

Similar words pop up in the word cloud such as `depopulation`, `amp`, and `vaccine`as before when we only accounted for October and November data. But this time, when considering all the data from October 2020 to March 2021, the most frequent term is `unvaccinated`. The increased discussion about vaccination makes sense since as of October 2020, the COVID-19 vaccine was developed and in the UK, the first vaccine was delivered in December 2020.

```{r lda_all}
dfmtrim_all <- dfm_all %>% 
              dfm_trim(min_docfreq = 0.01, max_docfreq = 0.8, docfreq_type = "prop")

set.seed(0)
tweets_lda_all <- textmodel_lda(dfmtrim_all, k = 10) 


DT::datatable(as.data.frame(terms(tweets_lda_all)), class = "display", rownames=FALSE, options = list(pageLength = 10, autoWidth = FALSE))
```

-   In Topic 6: Dr. Fauci is an American immunologist and "some falsely claimed that he was involved in creating the virus in a Chinese lab" [@wikiAF].

```{r}
lda_results_all <- as.data.frame(t(tweets_lda_all$phi))
```

#### Plotting

```{r lda_plt_all}
lda_results_all2 <- as_tibble(lda_results_all, rownames = NA) %>% 
                rownames_to_column(var = 'words') 
top_results(lda_results_all2)
plt_fn(toptopic1, toptopic2, toptopic3, toptopic4, toptopic5, toptopic6, toptopic7, toptopic8, toptopic9, toptopic10)

```

When considering all the data from October 2020 to March 2021, we recognize that the term `depopulation` does not appear as much as before in each topic. Instead, vaccination-related terms appeared more frequently (e.g., `vaccinated`, `unvaccinated` , `antivaccine`) Note again that `vaccinated` could mean `not vaccinated` in original tweets before tokenization. The following are the words that may represent each topic:

-   Topic 1: Vaccinate, Unvaccinated, People, Death, Data(Datum), Israel
-   Topic 2: Depopulation, Agendum, World, Reset, Amp
-   Topic 3: Unvaccinated, Child, Back, Teacher, School, Die
-   Topic 4: Work, Vaccine, Risk, Safe, High, Protect
-   Topic 5: Amp, Vaccine, Freedom, Health
-   Topic 6: Vaxxed, Dr.Facuci, Pain
-   Topic 7: Vaccine, Vaccination, Mask, Make, State
-   Topic 8: Vaccine, People, Covid, Kill, Test
-   Topic 9: Unvaccinated, Racist, Prepare, People, Food
-   Topic 10: Depopulation, Bill Gates, Antivaccine, Medical

Aligned with the results of the LDA with 2-months data sets, we again notice that all the topics seem to be linked to either conspiracy theory or people's vaccine hesitancy. However, given some newly occurring words that were not present in the previous analysis, we will inspect which topics prevail in each month.

## Topic per Month

To investigate what topics predominantly appear over the months, we will utilize the `theta` values that the LDA algorithm generated. These values show how each text is related to the 10 topics.

```{r}

lda_theta <- as.data.frame(tweets_lda_all$theta)
monthlytopic <- cbind(data_all_in[, c('created_at', 'cleaned_tweets_all')], lda_theta)
monthlytopic$textID <- rownames(monthlytopic)

DT::datatable(head(monthlytopic), class = "display", rownames=TRUE, options = list(
  pageLength = 5, autoWidth = FALSE))
```

```{r}
monthlytopic_long <- pivot_longer(monthlytopic, cols = 3:12, names_to = "Topic", values_to = "Weights" )
top_topic_tweets <- monthlytopic_long %>% 
        group_by(textID) %>% 
        filter(Weights == max(Weights)) %>% 
        arrange(textID, Topic)
# Since textID is a string, text1*... comes before text2*. 
```

```{r}
date <- parse_date(top_topic_tweets$created_at, format = "%a %b %d %H:%M:%S %z %Y", na=c("", "NA"), locale=default_locale(), trim_ws = TRUE)

top_topic_tweets$date <- date

top_topic_tweets <- top_topic_tweets %>% 
        select(textID, Topic, Weights, date)

DT::datatable(head(top_topic_tweets), class = "display", rownames=T, options = list(pageLength = 5, autoWidth = FALSE))
```

```{r}
top_topic_tweets$month <- format(top_topic_tweets$date, "%b") #Taking Months only

prop = top_topic_tweets %>% 
        group_by(month, Topic) %>% 
        summarise(n=n()) %>% 
        mutate(Freq = n/sum(n)) # proportion of Topic K per month

prop$Topic <- factor(prop$Topic, levels = c("topic1", "topic2", "topic3", "topic4", "topic5", "topic6", "topic7", "topic8", "topic9","topic10"))
```

```{r plt_months}
library(viridis)

p<-ggplot(prop, aes(x=month, y=Freq, fill=Topic)) + 
        geom_bar(stat = "identity", position = "stack")+
        scale_fill_viridis(discrete = TRUE, name= "Topics") + 
        scale_x_discrete(limits = c("Oct", "Nov", "Dec", "Jan","Feb", "Mar" )) +
        theme_minimal()
p

```

We see that **Topic 2,** which is about depopulation conspiracy theory with terms such as *Depopulation, Agendum, World, Reset, and Amp,* was the most discussed in October and then in November 2020. This goes in tandem with the introduction of the COVID-19 vaccine to the world. Similarly, **Topic 10**, which can be described by the terms, *Depopulation, Bill Gates, Antivaccine, and Medical*, is more discussed in 2020 than in 2021. **Topic 3** related to terms, *Unvaccinated, Child, Back, Teacher, School, and Die,* is most prominent March 2021 and then February 2021. This can be since many schools in the world start the academic year or start a semester after a break in February and March. **Topic 6**, concerning terms such as *vaxxed, leave, and Dr. Fauci*, show an ascending trend from October to December 2020 and then again from January 2021 to March 2021, while it occurred the most in March 2021. **Topic 9,** which is associated with terms such as *unvaccinated, racist, people, and food* appear most frequently in January 2021 while it is not so present in other months.

Other than these trends, we do not see an extreme variation across the months. To put it differently, all the topics are discussed in every month and no topic dominates the distribution extremely. Such a result may be due to the similarity between the topics as all of them are associated with conspiracy theory. Moreover, the fact that @muric2021a pre-selected the tweets with anti-vaccination sentiment can be a cause for the similarity between the topics. Considering this, we assess that it is not meaningful to label each tweet with different topics and perform a supervised topic analysis. Instead, we will use the insights we have gathered from analyzing the Twitter data in assessing the characteristics of the vaccine hesitancy with a survey data.

------------------------------------------------------------------------

# Study 2: Survey study

To complement the lack of personal information in the Twitter data, in the Study 2, we used survey data to explore the behind reasons why people oppose vaccines from a different perspective.

## Packages

```{r packages_ml}
# getwd()
# rm(list=ls())
####Load the packages####
#data manipulation 
library("haven") 
library("dplyr") 
library('base')

#data visualization
library(knitr)

#factor analysis
library(car)
library(psych)

#decision tree
library("rpart")
library("rpart.plot")
library(vip)

#random forest
library(randomForest)
library(caret)
library(e1071)
```

## Material and methods

### Data collection and processing

The data used in Study 2 - 'Living, Working and COVID-19 (round 3)' - was collected by Eurofound from EU-27 countries @ahrendt2020. The access of the preliminary data set was granted on 23 May 2022 by Eurofound.

Participants of this survey were recruited via uncontrolled convenience sampling, specifically by advertising on social media and distributing to Eurofound's stakeholders. The survey was released online via the SoSciSurvey platform. The round 3 data collection took place on 15 February 2021 and ended on 30 March 2021 in EU-27 countries.

Our accessed data set was already cleaned by Eurofound researchers to exclude partial interviews. The missing values (i.e., 'Don't know/Prefer not to answer', 'Not applicable' and skipped questions) have been coded as system missing.

In the current study, variables measured by Likert-scale with more than 5 levels were treated as continuous variables @knapp1990. Listwise deletion was used to deal with missing data.

### Outcome measure: vaccine hesitancy

Vaccine hesitancy was accessed in the survey by asking participants "*How likely or unlikely is it that you will take the COVID-19 vaccine when it becomes available to you?* " Participants were asked to reported their intention on a 5-point Likert scale from 1 *'very likely'* to 5 *'very unlikely'*.

Because we are interested in understanding why people are unwilling to receive vaccine, those who answered 3 'neither likely nor unlikely' were removed from our analysis for showing ambiguous attitudes. Then, we categorized respondents who answered '*very likely*' and '*rather likely*' as people without vaccine hesitancy, and categorized the respondents who answered 'rather unlikely' and 'very unlikely' as ones with vaccine hesitancy.

In our final sample, 23.7% respondents have vaccine hesitancy and 76.3% do not have it.

```{r load_data}
#load data
md <- read_sav("lwc_r1_r2_r3_public_09112021.sav")

##round 3 data with interested variables
mdv <- md %>% 
  filter(wave == 3,C314_01 != 3) %>%
  dplyr::select(
    #demographic
    C008, #Urbanization level
    B002, #gender
    B003_01, #age
    F344,#education
    D001,#employment status
    #well-being
    C001_01, #life satisfaction
    who5,
    #resilience
    C003_03,
    C003_04,
    #health
    C004_01,
    #trust 
    C007_01,
    C007_02,
    C007_03,
    C007_04,
    C007_05,
    C307_06,
    C307_07,
    C307_08,
    C312_01,
    #covid-19 experience
    C316_01,
    C316_02,
    C316_03,
    #media
    C319,
    #economic
    E006,
    #decision
    C314_01,
    #optimistic
    C003_01,
    #country
    B001,
    #tense
    C006_01
  )

#rename column
colnames(mdv) <- c('UrbanisationLevel', 'Gender', 'Age', 'Education', 'EmploymentStatus',
                   'LifeSatisfaction','WHO5',
                   'Resilience1','Resilience2',
                   'GeneralHealth',
                   'TrustNewsMedia','TrustPolice','TrustGov','TrustEU','TrustHealthcare','TrustSocialMedia', 'TrustScience','TrustPharFirm','TrustPeople',
                   'GotCovid','CloseGotCovid','CloseDieCovid',
                   'NewsSource',
                   'HouseSaving','AntiVac',
                   'SelfOptimistic','Country','Tense'
)

#sapply(mdv,attr,'label')
#category 1,2,4,5,20:25,27
#continuous 3,6:19,11:19,26,28

#set type of variables
mdv[c(1,2,4,5,20:25,27)] <- lapply(mdv[c(1,2,4,5,20:25,27)],factor)
mdv[c(3,6:19,26,28)] <- lapply(mdv[c(3,6:19,26,28)],as.numeric)
mdv$AntiVac <- ifelse(mdv$AntiVac %in% c('1','2'),'support','anti')
mdv$AntiVac <- as.factor(mdv$AntiVac)
mdv<-as.data.frame(na.omit(mdv))
table(mdv$AntiVac)
```

```{r }
# mdv data frame
DT::datatable(mdv, class = "display", rownames=FALSE, options = list(
  pageLength = 5, autoWidth = FALSE))
```

### Predictors measure

Several interested variables in the data set were selected to build a model that could predict individual's hesitancy of vaccination, and explore their importance in contributing vaccine hesitancy. We mainly focus on variables related with trust, well-being, COVID-19 experience and socioeconomic and demographic variables.

#### Predictor variables: trust

Trust was proved to correlates with COVID-19 vaccine hesitancy in many research [e.g., @schernhammer2022; @mewhirter2022]. One possible explanation is: the pandemic generated widespread disinformation that has undermines the acceptance and trust of science and policy, which extends to the issue of vaccine hesitancy @rawlings.

According to preliminary results of factor analysis, we divided trust into four factors: (1) trust in institutions: trust in police, trust in healthcare system, trust in government, trust in news media; (2) trust in science and pharmaceutical firms; (3) interpersonal trust: trust in people, trust in social media; (4) trust in European Union.

```{r}
KMO(r = cor(mdv[11:19]))
```

```{r}
t(cortest.bartlett(mdv[11:19]))
# KMO and Bartlett's test results indicate FA will be useful
```

```{r factor_analysis}
parallel <- fa.parallel(mdv[11:19])
#parallel analysis suggests that the number of factors = 4

fa.none <- fa(r=mdv[c(11:19)], 
 nfactors = 4, 
 # covar = FALSE, SMC = TRUE,
 fm='pa', # type of factor analysis we want to use (“pa” is principal axis factoring)
 max.iter=300,  
 rotate='varimax') # none rotation
fa.diagram(fa.none)
```

#### Predictor variables: well-being

Well-being variables were considered due to previous finding indicated that it is the anxiety and fear rather than hope driving people to take the vaccine @bullock2022. Therefore, in the current study, we take into account of optimistic, tense feeling, general life satisfaction, general subjective well-being, general health (general physical well-being) into account, to investigate whether they impact individual's vaccine hesitancy.

#### Predictor variables: COVID-19 experience

A person's experience with COVID-19 may influence their willingness to be vaccinated. For example, the death of someone around us because of COVID-19 may make us feel the danger of COVID-19, and thus more willing to get vaccinated. In the current study, we included three variables that related to COVID-19 experience. These variables accessed whether (1) one has tested positive of COVID-19, (2) one has close one tested positive of COVID-19, (3) one has close one died because of COVID-19.

#### Predictor variables: socioeconomic and demography

Based on previous findings, we consider a number of socioeconomic and demographic variables that are potentially associated with vaccine hesitancy @lee2022; @mewhirter2022. We included 2 socioeconomic variables (i.e., variables that reflect an individual's status within a community): employment status, house saving. For demographic variables, we included 5: urbanization level, education level, country, gender, age.

#### Predictor variables: news source, self-resilience

Finally, we considered news source and resilience as predictors of vaccine hesitancy. Online misinformation about COVID-19 vaccine was found linking to COVID-19 vaccination hesitancy and refusal @pierri2022. Compared to television programs, "news" published on social media are more likely to contain misinformation. For example, in Study 1, we found that anti-vaxxers often talk about topics related to conspiracy theory, and they are highly likely to see these mis- or unscientific information on social media (e.g., Twitter, Facebook) rather than from TV programs. Previous research has found that vaccine hesitancy was higher among those with high self-resilience than those with low self-resilience @schernhammer2022. Therefore, the self-resilience was also considered in this study. Self-resilience were accessed by two questions: (1) I find it difficult to deal with important problems that come up in my life, (2) When things go wrong in my life, it generally takes me a long time to get back to normal.

## Statistical models

Two supervised machine learning methods, `Decision Trees` and `Random Forest` , are used to investigate our research question. These two methods were considered because they can compensate for each other. Decision tree is a simple and effective decision-making diagram, which can handle mixed data (i.e., categorical and continuous data). While random forest is an extension of Bagging both of which improve decision trees. Although decision trees are easy to interpret and visualize, it has potential to over-fit the data or derive a model that is not accurate enough. Random forest compensates these issues of decision trees by yielding more accurate and precise results and by preventing the model from over-fitting as it uses multiple trees with bootstrap samples. However, interpretation is more difficult than the decision trees due to its complex visualization of the results and it requires more computational power.

### Train and test set preparation

In total, 30770 respondents were entered into the final analysis. We split the data into train- and test data sets, in the ratio of 75:25. 75% of the data serves to train and create the decision tree model and random forest model, while the rest make predictions and evaluate the accuracy of the model.

```{r splitting_data}
set.seed(377)

smp_size <- floor(0.75 * nrow(mdv))
train_ind <- sample(seq_len(nrow(mdv)), size = smp_size)
train <- mdv[train_ind, ]
test <- mdv[-train_ind, ]
```

### Decision trees

#### Modeling and pruning - with all variables

We first created a base model with low complexity parameter (`cp = 0`), which means that we allow decision tree to fully grow. Then, we pruned the model following Breiman's (1984) 1-SE rule (i.e., in practice, choose the simplest tree within one standard error of the best tree) [@breiman1984]. According to the output, the best tree is the one with lowest cross-validation relative error `xerror = 0.67244`. Based on 1SE rule, we chose the tree in row 8 (`xerror = 0.67919)`, which is lower than `0.67244 + 0.010151 = 0.682591`.

```{r base_DT}
#base model
#In R, we use 'rpart' package for "Recrusive Partitioning and Regression Trees". Because we want to predict whether a respondent is 'anti' or 'support' vaccination, so we chose method = "class".

set.seed(377)
tree.base <- rpart(AntiVac~.,train,method = 'class',control = rpart.control(cp=0))
printcp(tree.base)
#16 1.2767e-03     28   0.62356 0.67244 0.010151
```

Hence, we cut the tree with `cp = 3.1005e-03` in row 8, and compared the accuracy of base model and pruned model. From the output results, the pruned model has better accuracy (83.5%), which indicates that the pruned model is better suited for the production environment. Therefore, we decided the pruned model as our final decision tree model.

```{r pruned}
#pruned model
set.seed(377)
tree.prune <- rpart(AntiVac~.,train,method = 'class',control = rpart.control(cp=3.1005e-03))
#accuracy of base model
pred.base <- predict(tree.base, test,type='class')
base.mat <- table(test$AntiVac, pred.base)
base.accuracy <- sum(diag(base.mat)) / sum(base.mat)
#accuracy of pruned model
pred.prune <- predict(tree.prune, test,type='class')
prune.mat <- table(test$AntiVac, pred.prune)
prune.accuracy <- sum(diag(prune.mat)) / sum(prune.mat)

data.frame(base.accuracy,prune.accuracy)
```

The following shows the rules and the top 20 features of final decision tree model.

According to the rule of the decision plot, we can see from the root node on the top of the tree that 76% respondents among our final data do not have vaccine hesitancy, while 24% do. Travelling down the tree branches, we see that if respondents trust in pharmacy firm is higher than 3 out of 10, there are 87% respondents are likely to take vaccination, and so forth.

As demonstrated in the importance feature plot, trust variables and country play an important role in predicting vaccine hesitancy compared to well-being variables, COVID-experience variables, and other socioeconomic and demographic variables. Among trust variables, the trust in the pharmaceutical firms and science are the two most important features. The second is trust in government institutions and trust in EU. However, interpersonal trust appears rather less important.

These results are in line with the findings by @mewhirter2022. @mewhirter2022 also found that trust and some demographic variables (e.g., community) has strong association with vaccine hesitancy. However, they argued that researchers should consider deeper sources, instead of relying on demographic patterns when studying public health. It could be that people who distrust vaccine clustered in certain areas thus shaping demographic pattern. Considering this, we built a model with only trust in the pharmaceutical firms and science, trust in government institutions and trust in EU, and checked its accuracy.

```{r plot_DT}
rpart.plot(tree.prune)
vip(tree.prune, num_features = 20, bar = F)
```

#### Modeling and pruning - with trust variables

The same modeling and pruning process have done with a model with only trust variables. The results showed that the model with only trust variables (i.e., trust in the pharmaceutical firms and science, trust in government institutions and trust in EU) has the similar accuracy (82.85%) as the model with all variables (83.5%).

```{r base_DT_trust}
#base model - trust
set.seed(377)
tree.base.t <- rpart(AntiVac~ `TrustNewsMedia` + `TrustPolice` + `TrustGov` + `TrustEU` + `TrustHealthcare` + `TrustScience` + `TrustPharFirm`,train,method = 'class',control = rpart.control(cp=0))
printcp(tree.base.t)
        

#11 1.0943e-03     22   0.68886 0.71804 0.010422
# target: 0.728462 optimal cp = 2.5533e-03
```

```{r pruned_trust}
#pruned model
set.seed(377)
tree.prune.t <- rpart(AntiVac~ `TrustNewsMedia` + `TrustPolice` + `TrustGov` + `TrustEU` + `TrustHealthcare` + `TrustScience` + `TrustPharFirm`,train,method = 'class',control = rpart.control(cp=2.5533e-03))
#accuracy of base model
pred.base.t <- predict(tree.base.t, test,type='class')
base.mat.t <- table(test$AntiVac, pred.base.t)
base.accuracy.t <- sum(diag(base.mat.t)) / sum(base.mat.t)
#accuracy of pruned model
pred.prune.t <- predict(tree.prune.t, test,type='class')
prune.mat.t <- table(test$AntiVac, pred.prune.t)
prune.accuracy.t <- sum(diag(prune.mat.t)) / sum(prune.mat.t)

data.frame(base.accuracy.t,prune.accuracy.t,prune.accuracy)
```

Same as the model with all variables.

```{r plot_DT_trust}
rpart.plot(tree.prune.t)
vip(tree.prune.t, num_features = 20, bar = F)
```

We tried to model with only `trust in pharmaceutical firms` and `trust in science` as predictors and the result showed that the accuracy is 82.39%., which is very close to the previous model with more predictors (82.85%).

```{r base_DT_2trust}

#base model - with only 2 trust variables
set.seed(377)
tree.base.o <- rpart(AntiVac~`TrustPharFirm` + `TrustScience`,train,method = 'class',control = rpart.control(cp=0))
printcp(tree.base.o)
# target: 0.756339 optimal cp = 8.9367e-03

#pruned model - with only 2 trust variables
set.seed(377)
tree.prune.o <- rpart(AntiVac~`TrustPharFirm` + `TrustScience`,train,method = 'class',control = rpart.control(cp=8.9367e-03, minbucket = 5))

#accuracy of base model
pred.base.o <- predict(tree.base.o, test,type='class')
base.mat.o <- table(test$AntiVac, pred.base.o)
base.accuracy.o <- sum(diag(base.mat.o)) / sum(base.mat.o)
#accuracy of pruned model
pred.prune.o <- predict(tree.prune.o, test,type='class')
prune.mat.o <- table(test$AntiVac, pred.prune.o)
prune.accuracy.o <- sum(diag(prune.mat.o)) / sum(prune.mat.o)

data.frame(base.accuracy.o,prune.accuracy.o,prune.accuracy.t, prune.accuracy)

```

### Random Forests

#### Modeling and tuning - with all variables

We first run a default random forest model, and then tuned the default model to find a better solution.

The output of the default random forest model shows that 500 trees and tested 5 mtry values. The out-of-bag (OOB) error (i.e., prediction error of out-of-bags observations) is 14.46%. The `do.trace`argument allows us to see the change of OOB with the increase number of trees. As seen in the plot of OOB, the OBB has stabled when ntree = 500.

```{r random forest}
set.seed(377)
# Run the model
model.rf <- randomForest(`AntiVac`~., data = train, do.trace = T)
model.rf
```

```{r ntree}
###Step 2: tune the model 
#Find the best number of ntree
obb.error.data <- data.frame(Trees=rep(1:nrow(model.rf$err.rate),times = 3),
                             Type = rep(c('OOB','support','anti'),each=nrow(model.rf$err.rate)),
                             Error = c(model.rf$err.rate[,'OOB'],
                                       model.rf$err.rate[,'support'],
                                       model.rf$err.rate[,'anti']))
ggplot(data=obb.error.data,aes(x=Trees, y=Error))+
  geom_line(aes(color=Type)) 
```

Then, we turned to look at several possible mtry values in models. We tested the OOB for different mtry values (from 1 to 10). The results showed that when mtry = 10 the OOB is lowest. In the current study, we did not tuned for the maxnodes but allowed trees grew to the maximum possible value.

```{r mtry}
#Find the best number of mtry
oob.values <- vector(length = 10)
for(i in 1:10){
  temp.model <- randomForest(AntiVac~., data = train, ntree = 500, mtry=i)
  oob.values[i] <- temp.model$err.rate[nrow(temp.model$err.rate)]
}
order(oob.values) #choose mtry = 10
```

Lastly, we tuned the mtry value from 5 (default) to 10. And we reran model with tuned values. The results showed that the tuned model has an accuracy of 85.74%, which is higher than the default model (85.54%).

```{r final_RF}
model.rf.2 <- randomForest(AntiVac~., data = train, ntree = 500, mtry = 10)
model.rf.2
```

As done for decision tree model, we also check the confusion matrix and the accuracy of random forest model. The results showed that the accuracy of the random forest model (0.8584 \[0.8514, 0.867\] is better than decision tree model (0.835), but not too much.

```{r prediction}
rf.pred <- predict(model.rf.2, test)
rf.mat <- confusionMatrix(rf.pred, test$AntiVac)
rf.mat
```

Here we visualize the important variables for classifying the attitudes towards vaccination. Mean decrease in Gini measures how important a variable is for estimating the value of the target variable across all the three that make up the forest. Hence, the higher the variable's value of mean decrease Gini Score, the more important the variable is in the model.

The results we received from random forests are similar to what we have in decision trees. The trust in pharmaceutical firms plays the most important role in contributing vaccine hesitancy. Then follows the country and trust in science.

As we can see in the plot below, the demographic variables play rather important role than what we have seen in the decision tree.

```{r rule_RF}
varImpPlot(model.rf.2, n.var = 20)

```

#### Modeling and tuning - with trust variables

Similar to what we have done in the decision tree models, we only include `trust in pharmaceutical firms and science` as predictors and to see the accuracy of the model. The accuracy decreases by 3.55%.

```{r RF_trust}
set.seed(377)
# Run the model
model.rf.t <- randomForest(`AntiVac`~`TrustPharFirm` + `TrustScience`, data = train, do.trace = T)
model.rf.t

#compare the accuracy with model that entered all varaibles
rf.pred.t <- predict(model.rf.t, test)
rf.mat.t <- confusionMatrix(rf.pred.t, test$AntiVac)

data.frame(rf.mat$overall[1],rf.mat.t$overall[1])
```

## Conclusion

The following shows the accuracy of each model we have built.

```{r model_comparison}
ms <- data.frame(prune.accuracy, prune.accuracy.t, prune.accuracy.o, rf.mat.t$overall[1], rf.mat$overall[1])
ms <- ms %>%
  rename(
    DecisionTree.AllVar = prune.accuracy,
    DecisionTree.TrustVar = prune.accuracy.t,
    DecisionTree.2TrustVar = prune.accuracy.o,
    RandomForest.AllVar= rf.mat.overall.1.,
    RandomForest.2TrustVar = rf.mat.t.overall.1.
  )
```

```{r model_comparison_output}
ms
```

Conclusively, our findings indicated that while some demographic variables - country, age, education - associated with vaccine hesitancy, our results from decision trees and random forests suggested that `trust in pharmaceutical firms` and `trust in science` are two important trust that need to be addressed to in understanding individual's vaccine hesitancy.

# Final Conclusions & Future Directions

Why are some people unwilling to take or even against COVID-19 vaccines? We explored the data from Twitter (tweeted between 01 October 2020 and 31 March 2021) and from Eurofound (surveyed between 15 February 2021 and 30 March 2021) to answer this research question. We found from Study 1 with Twitter data that most anti-vaccination sentiment, which often coincides with vaccine hesitancy, is associated with the discussions about the conspiracy theory. The anti-vaccination tweets demonstrated people's belief that elites or people with power control the world and that COVID-19 or the vaccine is their agenda to depopulate the world. Study 2, with survey data, showed us that most vaccine hesitancy is associated with people's trust in pharmaceutical firms and science and, to a lesser extent, with people's trust in the government.

It is interesting to discover that the results of Study 1 and Study 2 resonate the debate on the definition of the terms *anti-vaccination* and *vaccination hesitancy*. For example, in the topic modeling with the Twitter data, we found words such as `depopulation`, `Bill Gates`, `Dr. Fauci`, `test`, and other conspiracy-theory-related terms, suggests that anti-vaxxers like to report distrust in a specific person or opinion. This narrow characterization of distrust reinforces @berman2020 argument that anti-vaxxers are a more extreme sample of people who are hesitant to vaccinate. Therefore, *anti-vaccination* and *vaccine hesitancy* can differ largely and may be treated separately. This falls into the limitation of our study in that we followed the definition of *vaccine hesitancy* that can be used interchangeability with *anti-vaccination*. Since the data of Study 1 had a focus on the anti-vaccination and the data of Study 2 covered rather a broader range of attitudes toward vaccination, the translation of results from Study 1 to Study 2 can be limited. Taking this into account, the same criticism is possible for the research by @muric2021a who also use the two terms interchangeably.

Each study also has some limitations. For Study 1, the tweets were pre-selected based on the keywords about anti-vaccination, which could influence the results of topic modeling, yielding similar topics. Moreover, analyzing the Twitter data, we realized that some accounts with strong anti-vaccination sentiment might have dominated the discussion. For instance, taking the data for all six months, the 359,688 tweets are posted by only 196,043 accounts, meaning that each of the 196,043 accounts posted 1.8 tweets on average. Consequently, the results of our topic analysis may be biased and cannot be fully translated as variables that explain one's vaccine hesitancy.

On the other hand, the survey data used in Study 2 has allowed a more direct tool to discover the variables to explain one's vaccine hesitancy. With the question targeted to inquiring into people's attitudes toward the COVID-19 vaccine, we could learn that a higher trust in pharmaceutical firms and science tends to associate with a lower degree of vaccine hesitancy and vice versa. However, the nature of the online survey excluded an older generation from the study as well as the fact that the respondents might not have been candid with their answers leaves some possibilities for bias. Another limitation of Study 2 is while the country variable appeared very important, we did not yet figure out why and how it matters for vaccine hesitancy. Moreover, we were unable to see the interaction effect of predictors.

Nonetheless, we observe that the results from the topic model with the Twitter data and the supervised machine learning model with the Survey data overlap to some degree. This suggests that increasing trust in institutions, including firms, government, and academia, is vital to mitigate people's vaccine hesitancy. When more people feel that a few elites or people with the power maneuver the world, the trust will not grow and vaccine hesitancy will persist. Even though we cannot statistically compare the two results due to differences in the data and the methods, this conclusion allows us to think about the source of vaccine hesitancy and how to combat it. For future research, we suggest delving further into the relationship between people's assessment of the government's performance or big companies' contribution to the society on the one hand and the evolution of their trust in institutions on the other hand to study vaccine hesitancy. We also suggest that future studies could examine whether conspiracy theories influence individual acceptance of vaccines, and we advocate to distinguish anti-vaxxers from those who are hesitant to take vaccine, especially when studying anti-vaccine movements or protests.

# References
